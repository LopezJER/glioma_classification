{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glioma Grading Classification based on Clinical and Genetic Features\n",
    "\n",
    "This notebook aims at properly classifying the two different grades of CNS glial cells cancer: low-grade gliomas (LGG) and high-grade gliomas (HGG). While the former is slowly progressing and usually does not require treatment, the latter is incredibly aggressive and frequently metastatizes. Thus, correct classification is pivotal to ensure each patient receives adequate care for their specific glioma type.\n",
    "\n",
    "The employed dataset is taken from the TCGA (the Cancer Genome Atlas) Program, which provides the 20 most frequently mutated genes in glioma patients, as well as several clinical features. \n",
    "\n",
    "In what follows, we will carry out:\n",
    "- Exploratory data analysis and data preprocessing;\n",
    "- Categorical dimensionality reduction;\n",
    "- Model selection out of a range of possible classifiers; \n",
    "- Improve the classifiers' performance through hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 0: Create venv and install necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1: import necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 2: import LogisticRegression from sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary import statements\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import OrderedDict\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn import tree \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV , cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, roc_curve, auc, recall_score\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us visualize the raw dataset. \n",
    "\n",
    "Immediately, we notice that the only type of High Grade Glioma is Glioblastoma Multiforme (GBMs), which is considered a grade IV astrocytoma. Further, the dataset does not distinguish between primary GBMs (arising de novo) and secondary GBMs (evolving from lower grades); thus, drawing molecular conclusions on the differences between the two will not be straightforward. Interestingly, this dataset classifies grade III gliomas as low-grade, which is in conflict with World Health Organization glioma grade classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3: import data from csv and display it\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we identify unique values and remove non-reported values. Further:\n",
    "    \n",
    "- We assign:\n",
    "  - A value of 1 to mutated genes;\n",
    "  - A value of 0 to non-mutated genes;\n",
    "  - A value of 1 to the very aggressive Glioblastoma Multiforme ('GBM');\n",
    "  - A value of 0 to the milder Low-grade Gliomas ('LGG');\n",
    "  - A value of 1 for females;\n",
    "  - A value of 0 for males.\n",
    "\n",
    "- We one-hot encode the race feature. Please note that this was the only one-hot encoded feature, whereas every gene was simply given a value of 0 or 1. This was done in an effort to not excessively increase dimensionality.\n",
    "\n",
    "- We turn all non numerical values in the 'Age_at_diagnosis' column into numerical values. For example, '51 years 108 days'\tbecomes 51.295890. As the age boxplot does not show outliers, we can impute the missing age values with the mean value (rather than the median). Finally, we normalize age values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 4: what should we do with the 'Case_ID', 'Project,' and 'Primary_Diagnosis' columns? How necessary are they for this problem?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.columns:\n",
    "    print(f'for column {i} unique values are: {df[i].unique()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 5: Replace categorical variables with binary values to feed to the models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing rows with not identified variables\n",
    "df = df[ (df['Gender'] != '--') & (df['Race'] != 'not reported')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 6: Drop unnecessary columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding for race column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting column 'Age_at_diagnosis' to numerical data\n",
    "years = df['Age_at_diagnosis'].str.split(' ', expand=True)\n",
    "years.drop(columns=[1, 3], inplace=True) # Dropping unnecessary columns (with words years and days)\n",
    "years[0] = pd.to_numeric(years[0], errors='coerce') # Converting years to numerical data\n",
    "years[2] = pd.to_numeric(years[2], errors='coerce') # Converting days to numerical data\n",
    "years[0] = years[0] + years[2] / 365 # Corrected this line\n",
    "df['Age_at_diagnosis'] = years[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing age values with mean and not median, as the boxplot shows no outliers\n",
    "df.rename(columns={'Age_at_diagnosis': 'Age'}, inplace=True)\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "# visualizing statistics of the data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 7: Plot distribution of age to see if there are any outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing age\n",
    "df['Age'] = (df['Age'] - df['Age'].mean()) / df['Age'].std()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we visualize data and inspect: \n",
    "- How gender distributes by age for the current dataset;\n",
    "- The prevalence of gliomas by gender.\n",
    "\n",
    "Notably, there are more men in the dataset than women, but age is relatively similarly distributed across genders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking age distribution by gender\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(x='Age', data=df, hue='Gender', palette={0: 'blue', 1: 'pink'}, hue_order=[0, 1], multiple='stack', bins=20, kde=True)\n",
    "plt.title('Age Distribution by gender')\n",
    "plt.legend(title='Gender', labels=['Women', 'Men'])\n",
    "plt.show()\n",
    "\n",
    "# Counting the number of each gender in the dataset\n",
    "df['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, the cell below shows that our data is in full accordance with current glioma literature. Specifically: \n",
    "- Men are more affected than women; \n",
    "- This gender imbalance is more prominent for high-grades gliomas. As a reminder, high-grade GBMs is given a value of 1, whereas the LGGs are given a value of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 8: Plot distribution of glioma by gender, color coding data based on gender\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genetic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we index the dataset to only visualize the 20 genetic features, specifically: \n",
    "- their mutation frequence; \n",
    "- the mutation distribution for each gene.\n",
    "\n",
    "Both tables point to mutations on the IDH1 gene being extremely common. This is not surprising, as this gene is termed the “trunk” (or initiating) event in the genesis of IDH-mutant gliomas.\n",
    "\n",
    "The other mutation patterns (such as the prevalent mutations on TPRX and ATRX genes) are coherent with the latest literature on glial cells tumorogenesis, despite the dataset not providing information as to whether a GBM is primary or secondary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of most commonly mutated genes in the dataset \n",
    "df['Most_commonly_mutated_gene'] = df.iloc[:, 3:-5].idxmax(axis=1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.countplot(data=df, y='Most_commonly_mutated_gene', palette='Set2')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Gene')\n",
    "plt.title('Distribution of most commonly mutated genes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mutation distribution for each gene\n",
    "fig = make_subplots(rows=5, cols=4)\n",
    "\n",
    "# Loop through columns 3 to 22 (indexing from 0 to 19)\n",
    "column_names = []  # List to store column names for legend\n",
    "\n",
    "for i, column in enumerate(df.iloc[:, 3:23].columns, start=1):\n",
    "    row_num = (i - 1) // 4 + 1  # Calculate row number\n",
    "    col_num = (i - 1) % 4 + 1   # Calculate column number\n",
    "\n",
    "    value_counts = df[column].value_counts()\n",
    "\n",
    "    # Map the index to the desired labels\n",
    "    x_labels = value_counts.index.map({0: 'Non-mutated', 1: 'Mutated'})\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=x_labels, y=value_counts.values),\n",
    "        row=row_num, col=col_num\n",
    "    )\n",
    "    \n",
    "    column_names.append(f'Column {column}')  # Store column names for legend\n",
    "\n",
    "    fig.update_xaxes(title_text=f'{column}', row=row_num, col=col_num)  \n",
    "\n",
    "# Update layout for all subplots\n",
    "fig.update_layout(title='Mutation Distribution for Each of the 20 Most Commonly Mutated Genes', width = 1000, height = 1200)\n",
    "fig.for_each_trace(lambda trace: trace.update(name=column_names.pop(0)))  # Update trace names using stored column names\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions of the Exploratory Data Analysis \n",
    "\n",
    "Thorough comparison of the dataset with the existing literature on gliomas was necessary in order to fully comprehend the task at hand.\n",
    "It appears that genetic and clinical information in the dataset aligns with what is currently known about LGGs and HGGs. Some variation is present, but we conclude that this may be due to statistical noise, sampling variability or the nature of TCGA. In fact: \n",
    "- the dataset only classifies GBMs as HGGs, which is no longer in accordance with the WHO guidelines;\n",
    "- the dataset does not distinguish between GBM subtypes. \n",
    "\n",
    "Although the EDA proved insightful, these factors should caution against drawing definitive conclusions from these findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "A variety of binary classification models will be trained, namely: Logistic Regression, DecisionTrees, Random Forest and AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove column 'most commonly mutated genes' from dataframe before fitting model, otherwise string error\n",
    "df = df.iloc[:, :-16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 9: Define label and features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 10: Split data into train and test sets, 70% train, 30% test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 11: Create a list of potential models: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 12: Train and evaluate the logistic regression model first\n",
    "# TODO 12a: Train the logistic regression model\n",
    "\n",
    "\n",
    "# TODO 12b: Evaluate on the test set\n",
    "\n",
    "# STEP 1: Get model predictions and store in variable 'y_pred'. Print y_pred to check.\n",
    "#y_pred=\n",
    "#print(y_pred)\n",
    "\n",
    "# STEP 2: Evaluate model based on accuracy and recall. Print accuracy and recall to check.\n",
    "#accuracy=\n",
    "#recall=\n",
    "#print(accuracy, recall)\n",
    "\n",
    "# STEP 3: Organize results in a confusion matrix, and analyze the results. Simply uncomment the lines below.\n",
    "# cm = confusion_matrix(y_test, y_pred, labels = model1.classes_)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = model1.classes_)\n",
    "# disp.plot()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 13: \n",
    "##### For each of the models, run the entire training and evaluation pipeline. Store accuracy and recall of models in a dictionary.\n",
    "\n",
    "# Step 1: Create dict for storing model's accuracy \n",
    "\n",
    "# Step 2: Loop through each of the models\n",
    "for model in models:\n",
    "    # Step 2a: Fit the model. \n",
    "\n",
    "    # Step 2b: Fetch model predictions and store in y_pred.\n",
    "\n",
    "    # Step 2c: Calculate and print accuracy and recall, respectively.\n",
    "\n",
    "    print(f\"Accuracy of {model} is {accuracy}\")\n",
    "    print(f'Recall (sensitivity) of {model} is {recall}')\n",
    "\n",
    "    # Step 2d: Store the accuracy of the model in the dictionary accordingly.\n",
    "\n",
    "\n",
    "    # Step 2d: Generate confusion matrix\n",
    "\n",
    "\n",
    "    print(\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 14: Get summary of results by checking the entire dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting first two models with the best recall\n",
    "sorted_dict = OrderedDict(sorted(dict.items(), key = itemgetter(1), reverse = True))\n",
    "first2pairs = {k: sorted_dict[k] for k in list(sorted_dict)[:2]}\n",
    "print(f'The two models with best recall are {first2pairs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing models from models list that didnt performe well\n",
    "keys = first2pairs.keys()\n",
    "models = [model for model in models if model in keys]\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation for both models, accuracy and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation (ACCURACY, keeping for paper comparison)\n",
    "splits = [5, 10] # Trying 5 and 10 folds\n",
    "dict_cv_accuracy = {} # Creating dict for storing model's accuracy\n",
    "for i in splits:\n",
    "    kf = StratifiedKFold(n_splits = i, shuffle = True) # random_state = 42)\n",
    "    for model in models:\n",
    "        name = '{}_{}'.format(model, i)\n",
    "        accuracy = cross_val_score(model, X_train, y_train, cv=kf, scoring='accuracy').mean()\n",
    "        dict_cv_accuracy[name] = accuracy\n",
    "        print(f'''\n",
    "                Accuracy of {model} for {i} splits is {round(accuracy.mean(), 5)} \n",
    "            ''')\n",
    "    print(\"                ---------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO 15: Cross-validate on all the models for RECALL.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting first two models with the best accuracy\n",
    "sorted_dict_cv = OrderedDict(sorted(dict_cv_accuracy.items(), key = itemgetter(1), reverse = True))\n",
    "sorted_dict_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting first two models with the best recall\n",
    "# sorted_dict_cv = OrderedDict(sorted(dict_cv_recall.items(), key = itemgetter(1), reverse = True))\n",
    "# sorted_dict_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch for Logistic Regression, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf_10 = StratifiedKFold(n_splits = 10, shuffle=True, random_state=42)\n",
    "kf_5 = StratifiedKFold(n_splits = 10, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV for logistic regression\n",
    "\n",
    "param_grid = {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': np.logspace(-4, 4, 20), #more granular params bc evenly dsitributed in logspace\n",
    "    'solver' : ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga']\n",
    "    }\n",
    "\n",
    "grid_search = GridSearchCV(estimator = models[0],\n",
    "                           param_grid = param_grid,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = kf_10).fit(X_train, y_train)\n",
    "\n",
    "# TODO 13: Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred with the best parameters from GridSearchCV\n",
    "y_pred_accuracy = best_model.predict(X_test)\n",
    "\n",
    "accuracy_gs = accuracy_score(y_test, y_pred_accuracy)\n",
    "y_pred = models[0].predict(X_test)\n",
    "prev_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Previous accuracy of LogisticRegression is {prev_accuracy}\")\n",
    "\n",
    "if accuracy_gs > prev_accuracy:\n",
    "    print(\"Accuracy improved after GridSearch\")\n",
    "    print(f'GridSearch accuracy is {accuracy_gs}')\n",
    "else: \n",
    "    print(\"Accuracy did not improve after gridsearch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearch for Logistic Regression, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomsearch for logistic regression\n",
    "\n",
    "param_grid = [    \n",
    "    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'solver' : ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga']\n",
    "    }\n",
    "]\n",
    "\n",
    "randomized_search = RandomizedSearchCV(estimator = models[0],\n",
    "                           param_distributions = param_grid,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = kf_10).fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model_rs = randomized_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters: {randomized_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {randomized_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_accuracy_rs = best_model_rs.predict(X_test)\n",
    "accuracy_rs = accuracy_score(y_test, y_pred_accuracy_rs)\n",
    "\n",
    "y_pred = models[0].predict(X_test)\n",
    "prev_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Previous accuracy of LogisticRegression is {prev_accuracy}\")\n",
    "\n",
    "if accuracy_rs > prev_accuracy:\n",
    "    print(\"Accuracy improved after Random Search\")\n",
    "    print(f'Random Search accuracy is {accuracy_rs}')\n",
    "else: \n",
    "    print(\"Accuracy did not improve after Random Search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch for logistic regression, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearchCV for logistic regression\n",
    "\n",
    "param_grid = [    \n",
    "    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': np.logspace(-4, 4, 20), #more granular params bc evenly dsitributed in logspace\n",
    "    'solver' : ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga']\n",
    "    }\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = models[0],\n",
    "                           param_grid = param_grid,\n",
    "                           scoring = 'recall',\n",
    "                           cv = kf_10).fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred with the best recall parameters from GridSearchCV\n",
    "y_pred_recall = best_model.predict(X_test)\n",
    "\n",
    "recall_gs = recall_score(y_test, y_pred_recall)\n",
    "cm = confusion_matrix(y_test, y_pred_recall, labels = models[0].classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = models[0].classes_)\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = models[0].predict(X_test)\n",
    "prev_accuracy = recall_score(y_test, y_pred)\n",
    "print(f\"Previous recall of LogisticRegression is {prev_accuracy}\")\n",
    "\n",
    "if recall_gs > prev_accuracy:\n",
    "    print(\"Recall improved after GridSearch\")\n",
    "    print(f'Random Search recall is {recall_gs}')\n",
    "else: \n",
    "    print(\"Recall did not improve after Random Search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearch for Logistic Regression, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomsearch for logistic regression\n",
    "\n",
    "param_grid = [    \n",
    "    {'penalty' : ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'C': np.logspace(-4, 4, 20),\n",
    "    'solver' : ['lbfgs', 'newton-cg', 'liblinear', 'sag', 'saga']\n",
    "    }\n",
    "]\n",
    "\n",
    "randomized_search = RandomizedSearchCV(estimator = models[0],\n",
    "                           param_distributions = param_grid,\n",
    "                           scoring = 'recall',\n",
    "                           cv = kf_10).fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model_rs = randomized_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters: {randomized_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {randomized_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now use best parameters from random search to predict y_pred\n",
    "\n",
    "y_pred_recall_rs = best_model_rs.predict(X_test)\n",
    "\n",
    "recall_rs = accuracy_score(y_test, y_pred_recall_rs)\n",
    "cm_rs = confusion_matrix(y_test, y_pred_recall_rs, labels = models[0].classes_) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm_rs, display_labels = models[0].classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = models[0].predict(X_test)\n",
    "prev_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Previous recall of LogisticRegression is {prev_accuracy}\")\n",
    "\n",
    "if recall_rs > prev_accuracy:\n",
    "    print(\"Recall improved after Random Search\")\n",
    "    print(f'Random Search recall is {recall_rs}')\n",
    "else: \n",
    "    print(\"Recall did not improve after Random Search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch for AdaBoost, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.1, 0.5, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = models[3],\n",
    "                           param_grid = param_grid,\n",
    "                           scoring = 'recall',\n",
    "                           cv = kf_10).fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred with the best recall parameters from GridSearchCV\n",
    "y_pred_recall = best_model.predict(X_test)\n",
    "\n",
    "recall_gs = recall_score(y_test, y_pred_recall)\n",
    "cm = confusion_matrix(y_test, y_pred_recall, labels = models[3].classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = models[3].classes_)\n",
    "disp.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = models[3].predict(X_test)\n",
    "prev_accuracy = recall_score(y_test, y_pred)\n",
    "print(f\"Previous recall of {models[3]} is {prev_accuracy}\")\n",
    "\n",
    "if recall_gs > prev_accuracy:\n",
    "    print(\"Recall improved after GridSearch\")\n",
    "    print(f'Random Search recall is {recall_gs}')\n",
    "else: \n",
    "    print(\"Recall did not improve after Grid Search\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomSearch for AdaBoost, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomSearch for SVM\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.1, 0.5, 1.0],\n",
    "    'algorithm': ['SAMME', 'SAMME.R']\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator = models[3],\n",
    "                           param_distributions = param_grid,\n",
    "                           scoring = 'recall',\n",
    "                           cv = kf_10).fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {random_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now use best parameters from random search to predict y_pred\n",
    "\n",
    "y_pred_recall_rs = best_model_rs.predict(X_test)\n",
    "\n",
    "\n",
    "recall_rs = recall_score(y_test, y_pred_recall_rs)\n",
    "cm_rs = confusion_matrix(y_test, y_pred_recall_rs, labels = models[3].classes_) \n",
    "disp = ConfusionMatrixDisplay(confusion_matrix = cm_rs, display_labels = models[3].classes_)\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = models[3].predict(X_test)\n",
    "prev_recall = recall_score(y_test, y_pred)\n",
    "print(f\"Previous recall of {models[3]} is {prev_accuracy}\")\n",
    "\n",
    "if recall_rs > prev_recall:\n",
    "    print(\"Recall improved after Random Search\")\n",
    "    print(f'Random Search accuracy is {recall_rs}')\n",
    "else: \n",
    "    print(\"Recall did not improve after Random Search\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
